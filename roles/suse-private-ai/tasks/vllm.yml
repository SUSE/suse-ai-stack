- name: Copy chat template jinja template
  copy:
    src: files/template_chatml.jinja
    dest: ~/template_chatml.jinja

- name: Get GPU node count in the cluster to tweak vllm_config memory utilization
  shell: |
    kubectl get nodes -l "nvidia.com/gpu.present=true" -o jsonpath="{.items[*].metadata.name}" --no-headers | wc -w
  register: get_gnodes

- name: Set GPU present node count in cluster
  set_fact:
    gpu_present_node_count: "{{ get_gnodes.stdout | int }}"

- name: Set Model Name extracted from model url
  set_fact:
    vllm_model_name: "{{ vllm_model_url.split('/')[-1][:7] | lower }}"

- name: Copy vllm-values.yaml
  template:
    src: vllm-values.yaml.j2
    dest: ~/vllm-values.yaml

- name: Deploy vllm helm chart
  shell: |
    if [[ {{ vllm_helm_repo }} == *"dp.apps.rancher.io"* ]]; then
        helm registry login dp.apps.rancher.io/charts -u {{ application_collection_user_email }} -p {{ application_collection_user_token }}
        helm upgrade --install {{ vllm_release_name }} {{ vllm_helm_repo }} -n {{ suse_private_ai_namespace }} --create-namespace  --version {{ vllm_helm_version }} -f vllm-values.yaml
    else
        helm upgrade --install {{ vllm_release_name }} {{ vllm_chart_name }} --repo {{ vllm_helm_repo }} -n {{ suse_private_ai_namespace }} --create-namespace --version {{ vllm_helm_version }} -f vllm-values.yaml
    fi
  register: vllm_deploy_result
  until: vllm_deploy_result.rc == 0
  retries: 5
  delay: 30
  when: not deploy_rancher_only | default(False) | bool

- name: Wait for vllm to be successfully rolled out
  shell: |
    kubectl -n {{ suse_private_ai_namespace }} rollout status deploy/{{ item }}
  register: rollout_result
  retries: 30
  delay: 40
  until: "'successfully rolled out' in rollout_result.stdout"
  with_items:
  - "{{ vllm_release_name }}-deployment-router"
  - "{{ vllm_release_name }}-{{ vllm_model_name }}-deployment-vllm"
  when: not deploy_rancher_only | default(False) | bool
