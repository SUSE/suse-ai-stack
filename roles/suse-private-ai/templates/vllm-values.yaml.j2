global:
  imagePullSecrets:
  - {{ appco_secret }}
servingEngineSpec:
  modelSpec:
  - name: "{{ vllm_model_name }}"
{% if "dp.apps.rancher.io" in  vllm_helm_repo  %}
    registry: dp.apps.rancher.io
    repository: containers/lmcache-vllm-openai
    tag: {{ vllm_cache_image_version }}
    imagePullPolicy: "IfNotPresent"
{% else %}
    repository: "lmcache/vllm-openai"
    tag: "latest"
    imagePullPolicy: "IfNotPresent"
{% endif %}
    modelURL: "{{ vllm_model_url }}"
{% if "opt-125" in vllm_model_name  %}
    chatTemplate: "chat.jinja2"
    # chat template for opt-125m
    chatTemplateConfigMap: "{{ lookup('file', 'files/template_chatml.jinja') }}"
{% endif %}
    replicaCount: 1
{% if gpu_present_node_count | int < 2 %}
    requestCPU: 1
    requestMemory: "8Gi"
    requestGPU: 1
    vllmConfig:
      extraArgs: ["--disable-log-requests", "--gpu-memory-utilization", "0.8"]
{% else %}
    requestCPU: 2
    requestMemory: "8Gi"
{% endif %}
routerSpec:
{% if "dp.apps.rancher.io" in  vllm_helm_repo  %}
  tag: {{ vllm_cache_router_version }}
{% else %}
  repository: "lmcache/lmstack-router"
  tag: "latest" 
  imagePullPolicy: "IfNotPresent"
{% endif %}
  resources:
    requests:
      cpu: 1
      memory: "8Gi"
