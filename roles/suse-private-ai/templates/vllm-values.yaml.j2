global:
  imagePullSecrets:
  - {{ appco_secret }}
servingEngineSpec:
  modelSpec:
  - name: "{{ vllm_model_name }}"
{% if "dp.apps.rancher.io" in  vllm_helm_repo  %}
    registry: dp.apps.rancher.io
    repository: containers/lmcache-vllm-openai
    tag: {{ vllm_cache_image_version }}
    imagePullPolicy: "IfNotPresent"
{% else %}
    repository: "lmcache/vllm-openai"
    tag: "latest"
    imagePullPolicy: "IfNotPresent"
{% endif %}
    modelURL: "{{ vllm_model_url }}"
    chatTemplate: "chat.jinja2"
    # chat template for opt-125m
    chatTemplateConfigMap: "{{ lookup('file', 'files/template_chatml.jinja') }}"
    replicaCount: 1
{% if gpu_present_node_count | int < 2 %}
    requestCPU: 1
    requestMemory: "8Gi"
    requestGPU: 1
    vllmConfig:
      extraArgs: ["--disable-log-requests", "--enforce-eager", "--gpu-memory-utilization", "0.8"]
{% else %}
    requestCPU: 2
    requestMemory: "16Gi"
{% endif %}
routerSpec:
{% if "dp.apps.rancher.io" in  vllm_helm_repo  %}
  registry: dp.apps.rancher.io
  repository: containers/lmcache-vllm-openai
  tag: {{ vllm_cache_image_version }}
  imagePullPolicy: "IfNotPresent"
{% else %}
  repository: "lmcache/lmstack-router"
  tag: "latest" 
  imagePullPolicy: "IfNotPresent"
{% endif %}
